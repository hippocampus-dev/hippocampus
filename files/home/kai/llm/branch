#!/usr/bin/env bash

set -eo pipefail

OPENAI_HOST=${OPENAI_HOST:-"https://api.openai.com"}
# ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
MODEL=${MODEL:-"gpt-4o"}
# What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
#
# We generally recommend altering this or top_p but not both.
TEMPERATURE=${TEMPERATURE:-1}
# An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of #the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are# considered.
#
# We generally recommend altering this or temperature but not both.
TOP_P=${TOP_P:-1}
# How many chat completion choices to generate for each input message.
N=${N:-1}
# The maximum number of tokens to generate in the chat completion.
#
# The total length of input tokens and generated tokens is limited by the model's context length.
MAX_TOKENS=${MAX_TOKENS:-"null"}
# Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
#
# [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)
PRESENCE_PENALTY=${PRESENCE_PENALTY:-0}
# Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
#
# [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)
FREQUENCY_PENALTY=${FREQUENCY_PENALTY:-0}

system=$(cat <<'EOS'
Please create an appropriate branch name for the given diff according to the following specifications.

---

## Specifications

1. The branch name must be kept within 30 characters (MUST).
2. The branch name must be written in English (MUST).
EOS
)

u1=$(cat -)

if [ -z "$u1" ]; then
  exit 0
fi

system=$(jq -n --arg role "system" --arg content "$system" '{"role": $role, "content": $content}')
u1=$(jq -n --arg role "user" --arg content "$u1" '{"role": $role, "content": $content}')

response=$(curl -fsSL $OPENAI_HOST/v1/chat/completions \
  -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d "$(echo $system $u1 | jq -sc \
    --arg model "$MODEL" \
    --argjson temperature "$TEMPERATURE" \
    --argjson top_p "$TOP_P" \
    --argjson n "$N" \
    --argjson max_tokens "$MAX_TOKENS" \
    --argjson presence_penalty "$PRESENCE_PENALTY" \
    --argjson frequency_penalty "$FREQUENCY_PENALTY" \
    '. as $messages | {"model": $model, "messages": $messages, "temperature": $temperature, "top_p": $top_p, "n": $n, "max_tokens": $max_tokens, "presence_penalty": $presence_penalty, "frequency_penalty": $frequency_penalty}' \
  )"
)

if [ "$(echo $response | jq -r '.error')" != "null" ]; then
  echo $response | jq -r '.error' 1>&2
  exit 1
fi
if [ $N -gt 1 ]; then
  echo $response | jq -c '.choices[]' | fzf --preview='echo {} | jq -r .message.content' --preview-window wrap | jq -r .message.content
else
  echo $response | jq -r '.choices[0].message.content'
fi
