{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f38569-007f-48c4-a536-515cd75b9f91",
   "metadata": {
    "jupyter": {
     "editable": false,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": "import asyncio\nimport collections.abc\nimport dataclasses\nimport logging\nimport math\nimport os\nimport tempfile\nimport typing\nimport warnings\n\nimport IPython.display\nimport ipyvuetify.extra\nimport ipywidgets\nimport nest_asyncio\nimport openai\nimport openai.types.audio\nimport openai.types.chat\nimport pydantic\n\ntry:\n    import pydub\n    import pydub.silence\nexcept ImportError:\n    import pip\n\n    pip.main([\"install\", \"pydub\"])\n    import pydub\n    import pydub.silence\n\ntry:\n    import pyannote.audio\nexcept ImportError:\n    import pip\n\n    pip.main([\"install\", \"pyannote.audio\"])\n    import pyannote.audio\n\n\nclass DialogueSegment(pydantic.BaseModel):\n    speaker: typing.Literal[\"タカシ\", \"タケシ\"]\n    content: str\n\n\nclass ResponseFormat(pydantic.BaseModel):\n    title: str\n    dialogues: typing.List[DialogueSegment]\n\n\nwarnings.simplefilter(\"ignore\")\nnest_asyncio.apply()\nlogging.getLogger(\"httpx\").setLevel(logging.ERROR)\n\nCOMBINED_TAB_NAME = \"Combined\"\nCACHE_DIRECTORY = os.path.expanduser(\"~/work/.cache/transcribe\")\nOPENAI_BASE_URL = \"http://cortex-api.cortex-api.svc.cluster.local:8080/v1\"\nwith open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\", \"r\", encoding=\"utf-8\") as f:\n    OPENAI_API_KEY = f.read()\n\nfile_upload = ipyvuetify.extra.FileInput()\n\nmodel = ipywidgets.widgets.Dropdown(\n    options=[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"],\n    value=\"gpt-4o-mini-transcribe\",\n    description=\"Model: \",\n)\n\nprompt = ipywidgets.widgets.Textarea(\n    value=\"\",\n    description=\"Prompt: \",\n)\n\ndiarization = ipywidgets.widgets.Checkbox(\n    value=False,\n    description=\"Diarization\",\n    disabled=True,\n)\n\nspeakers = ipywidgets.widgets.IntSlider(\n    value=2,\n    min=1,\n    max=10,\n    step=1,\n    description=\"Speakers:\",\n    disabled=True,\n)\n\nprocess_button = ipywidgets.widgets.Button(\n    description=\"Process\",\n    button_style=\"primary\",\n    icon=\"play\",\n)\n\noutput = ipywidgets.widgets.Output()\nstatus_label = ipywidgets.widgets.Label(value=\"\")\n\n\nasync def post_ephemeral_message(\n    message: str,\n    duration: int = 1,\n):\n    status_label.value = message\n    await asyncio.sleep(duration)\n    status_label.value = \"\"\n\n\nasync def extract_audio_from_video(working_directory: str, video_path: str, output_format: str = \"mp3\") -> str:\n    output_path = os.path.join(working_directory, f\"{video_path}.{output_format}\")\n\n    def _extract() -> str:\n        audio = pydub.AudioSegment.from_file(video_path)\n        audio.export(output_path, format=output_format)\n        return output_path\n\n    return await asyncio.get_running_loop().run_in_executor(None, _extract)\n\n\nasync def split_audio_by_size(\n    working_directory: str,\n    audio_path: str,\n    max_size_mb: int,\n    output_format: str = \"mp3\",\n) -> collections.abc.Sequence[str]:\n    audio = pydub.AudioSegment.from_file(audio_path)\n\n    file_size = os.path.getsize(audio_path)\n\n    max_size_bytes = max_size_mb * 1024 * 1024\n    number_of_files = math.ceil(file_size / max_size_bytes)\n\n    if number_of_files <= 1:\n        return [audio_path]\n\n    length = len(audio) // number_of_files\n\n    futures = []\n    for i in range(number_of_files):\n        start = i * length\n        end = min((i + 1) * length, len(audio))\n\n        chunk = audio[start:end]\n        chunk_path = os.path.join(working_directory, f\"{audio_path}_{i:03d}.{output_format}\")\n\n        def _export(c=chunk, cp=chunk_path) -> str:\n            c.export(cp, format=output_format)\n            return cp\n\n        futures.append(asyncio.get_running_loop().run_in_executor(None, _export))\n\n    return await asyncio.gather(*futures)\n\n\nasync def split_audio_by_silence(\n    working_directory: str,\n    audio_path: str,\n    max_size_mb: int,\n    output_format: str = \"mp3\",\n) -> collections.abc.Sequence[str]:\n    audio = pydub.AudioSegment.from_file(audio_path)\n\n    file_size = os.path.getsize(audio_path)\n    max_size_bytes = max_size_mb * 1024 * 1024\n\n    total_ms = len(audio)\n    target_ms = int(max_size_bytes * total_ms / file_size)\n\n    silences = pydub.silence.detect_silence(audio)\n    cuts = sorted({(s + e) // 2 for s, e in silences})\n    boundaries = [0] + cuts + [total_ms]\n\n    chunks = []\n    i = 0\n    while i < len(boundaries) - 1:\n        j = i + 1\n        while j < len(boundaries) and boundaries[j] - boundaries[i] <= target_ms:\n            j += 1\n\n        start_ms = boundaries[i]\n        end_ms = boundaries[j - 1]\n        if end_ms == start_ms:\n            end_ms = min(start_ms + target_ms, total_ms)\n\n        chunks.append((start_ms, end_ms))\n\n        if end_ms not in boundaries:\n            boundaries.insert(i + 1, end_ms)\n        i = boundaries.index(end_ms)\n\n    futures = []\n    for i, (start_ms, end_ms) in enumerate(chunks):\n        chunk = audio[start_ms:end_ms]\n        chunk_path = os.path.join(working_directory, f\"{audio_path}_{i:03d}.{output_format}\")\n\n        def _export(c=chunk, cp=chunk_path) -> str:\n            c.export(cp, format=output_format)\n            return cp\n\n        futures.append(asyncio.get_running_loop().run_in_executor(None, _export))\n\n    return await asyncio.gather(*futures)\n\n\n@dataclasses.dataclass\nclass DiarizationChunk:\n    start: float\n    end: float\n    label: str\n    audio_path: str\n\n\nasync def perform_diarization(\n    working_directory: str,\n    audio_path: str,\n    num_speakers: int,\n    output_format: str = \"mp3\",\n) -> collections.abc.Sequence[DiarizationChunk]:\n    pipeline = pyannote.audio.Pipeline.from_pretrained(\n        \"pyannote/speaker-diarization-3.1\",\n    )\n\n    annotation = pipeline(audio_path, num_speakers=num_speakers)\n\n    futures = []\n    audio = pydub.AudioSegment.from_file(audio_path)\n    for segment, _, speaker in annotation.itertracks(yield_label=True):\n        start_ms = int(segment.start * 1000)\n        end_ms = int(segment.end * 1000)\n\n        chunk = audio[start_ms:end_ms]\n        chunk_path = os.path.join(working_directory, f\"{audio_path}_{start_ms}_{end_ms}_{speaker}.{output_format}\")\n\n        def _export(c=chunk, cp=chunk_path, s=segment, label=speaker) -> DiarizationChunk:\n            c.export(cp, format=output_format)\n            diarization_chunk = DiarizationChunk(\n                start=s.start,\n                end=s.end,\n                label=label,\n                audio_path=cp\n            )\n            return diarization_chunk\n\n        futures.append(asyncio.get_running_loop().run_in_executor(None, _export))\n\n    return await asyncio.gather(*futures)\n\n\nasync def transcribe_file(\n    file_path: str,\n) -> str:\n    file_path = os.path.abspath(file_path)\n    extension = os.path.splitext(file_path)[1].lower() or \".mp3\"\n\n    with tempfile.TemporaryDirectory() as t:\n        cache_directory = CACHE_DIRECTORY\n        if diarization.value:\n            cache_directory = os.path.join(cache_directory, \"diarization\")\n        cache_path = os.path.join(cache_directory, os.path.basename(file_path))\n        if os.path.exists(cache_path):\n            return open(cache_path).read()\n        else:\n            os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n\n        if extension in [\".mp4\"]:\n            output_format = \"mp3\"\n            file_path = await extract_audio_from_video(t, file_path, output_format=output_format)\n        else:\n            output_format = extension.lstrip(\".\")\n\n        async def atranscribe_by_openai(audio_path: str) -> openai.types.audio.Transcription:\n            client = openai.AsyncOpenAI(\n                base_url=OPENAI_BASE_URL,\n                api_key=OPENAI_API_KEY,\n            )\n\n            with open(audio_path, \"rb\") as audio_file:\n                response = await client.audio.transcriptions.create(\n                    model=model.value,\n                    file=audio_file\n                )\n\n            return response\n\n        if diarization.value:\n            speaker_chunks = await perform_diarization(t, file_path, speakers.value)\n            transcription_tasks = [atranscribe_by_openai(chunk.audio_path) for chunk in speaker_chunks]\n            transcriptions = await asyncio.gather(*transcription_tasks)\n\n            result = \"\"\n            for chunk, transcription in zip(speaker_chunks, transcriptions):\n                result += f\"{chunk.label} ({chunk.start:.2f}s - {chunk.end:.2f}s):\\n{transcription.text}\\n\\n\"\n\n            with open(cache_path, \"w\") as f:\n                f.write(result)\n\n            return result\n        else:\n            chunks = await split_audio_by_silence(t, file_path, 5, output_format=output_format)\n            transcription_tasks = [atranscribe_by_openai(chunk_path) for chunk_path in chunks]\n            transcriptions = await asyncio.gather(*transcription_tasks)\n\n            result = \"\\n\".join([t.text for t in transcriptions])\n\n            with open(cache_path, \"w\") as f:\n                f.write(result)\n\n            return result\n\n\nasync def transcribe_files() -> None:\n    process_button.disabled = True\n\n    try:\n        with tempfile.TemporaryDirectory() as t:\n            tasks = []\n            file_names = []\n\n            for uploaded in file_upload.get_files():\n                file_path = os.path.join(t, uploaded[\"name\"])\n                file_names.append(uploaded[\"name\"])\n                with open(file_path, \"wb\") as f:\n                    f.write(uploaded[\"file_obj\"].read())\n                tasks.append(transcribe_file(file_path))\n\n            if tasks:\n                transcriptions = await asyncio.gather(*tasks)\n\n                initialize_tabs()\n\n                if len(transcriptions) > 1:\n                    combined_text = []\n                    for i, transcription in enumerate(transcriptions):\n                        combined_text.append(f\"# {file_names[i]}\\n{transcription}\\n\")\n\n                    tab_contents[COMBINED_TAB_NAME].result.value = \"\\n\".join(combined_text)\n                else:\n                    tab_contents[COMBINED_TAB_NAME].result.value = \"\".join(transcriptions)\n\n                for i, file_name in enumerate(file_names):\n                    tab_contents[file_name] = TabContent(file_name)\n                    tab_contents[file_name].result.value = transcriptions[i]\n                    tabs.children += (tab_contents[file_name].widgets,)\n                    tabs.set_title(i + 1, file_name)\n            else:\n                status_label.value = \"No such file\"\n    except Exception as e:\n        status_label.value = f\"Error: {str(e)}\"\n        raise e\n    finally:\n        process_button.disabled = False\n\n\n@output.capture()\ndef process(b: ipywidgets.widgets.Button) -> None:\n    status_label.value = \"\"\n    output.clear_output()\n\n    task = asyncio.create_task(transcribe_files())\n    asyncio.get_running_loop().run_until_complete(task)\n\n\nprocess_button.on_click(process)\n\n\nclass TabContent:\n    def __init__(self, name: str):\n        self.name = name\n        self.result = ipywidgets.widgets.Textarea(\n            value=\"\",\n            description=\"Result: \",\n            layout=ipywidgets.Layout(width=\"80%\", height=\"300px\"),\n        )\n\n        self._clipboard_button = ipywidgets.widgets.Button(\n            description=\"\",\n            icon=\"clipboard\",\n            tooltip=\"Copy to clipboard\",\n            disabled=True,\n            layout=ipywidgets.Layout(width=\"40px\", height=\"40px\"),\n        )\n\n        self._clipboard_button.on_click(self.clipboard_handler)\n\n        self._summary_button = ipywidgets.widgets.Button(\n            description=\"Summary\",\n            icon=\"pen\",\n            disabled=True,\n        )\n        self._podcast_button = ipywidgets.widgets.Button(\n            description=\"Podcast\",\n            icon=\"headphones\",\n            disabled=True,\n        )\n\n        self._summary_button.on_click(lambda b: self.summary_handler(b))\n        self._podcast_button.on_click(lambda b: self.podcast_handler(b))\n\n        self.widgets = ipywidgets.widgets.VBox([\n            ipywidgets.widgets.HBox([\n                self.result,\n                self._clipboard_button\n            ]),\n            ipywidgets.widgets.HBox([\n                self._summary_button,\n                self._podcast_button,\n            ])\n        ])\n\n        self.result.observe(self.update_button_availability, names=\"value\")\n\n    def update_button_availability(self, change) -> None:\n        has_content = bool(change.new)\n\n        self._clipboard_button.disabled = not has_content\n        self._summary_button.disabled = not has_content\n        self._podcast_button.disabled = not has_content\n\n    @output.capture()\n    def clipboard_handler(self, b: ipywidgets.widgets.Button) -> None:\n        IPython.display.display(IPython.display.Javascript(f\"navigator.clipboard.writeText(`{self.result.value}`);\"))\n        asyncio.create_task(post_ephemeral_message(\"Copied to clipboard\"))\n\n    @output.capture()\n    def summary_handler(self, b: ipywidgets.widgets.Button) -> None:\n        b.disabled = True\n\n        client = openai.OpenAI(\n            base_url=OPENAI_BASE_URL,\n            api_key=OPENAI_API_KEY,\n        )\n\n        response = client.chat.completions.create(\n            model=\"gpt-5.2\",\n            messages=[\n                openai.types.chat.ChatCompletionSystemMessageParam(\n                    role=\"system\",\n                    content=\"You are an AI assistant that summarizes the transcript of an audio file.\"\n                ),\n                openai.types.chat.ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=self.result.value,\n                )\n            ],\n        )\n\n        content = response.choices[0].message.content\n        self.result.value = content\n\n        b.disabled = False\n\n    async def _podcast_handler(self):\n        async def generate_speech_segment(client, speaker, text, segment_file):\n            voice = \"onyx\" if speaker == \"タカシ\" else \"echo\"\n            instructions = \"\"\"\n落ち着いた男性の声。分析的で理知的なトーン。明瞭で説得力のある話し方。\n専門知識を持つ解説者のような声質。少しゆっくりめのテンポ。\n\"\"\" if speaker == \"タカシ\" else \"\"\"\n明るく活発な男性の声。表現力豊かで感情を込めた話し方。\nユーモアのある軽快なトーンと抑揚。親しみやすく、エネルギッシュな声質。\nテンポ良く会話するスタイル。\n\"\"\"\n\n            response = await client.audio.speech.create(\n                model=\"gpt-4o-mini-tts\",\n                voice=voice,\n                input=text,\n                instructions=instructions,\n                response_format=\"mp3\",\n            )\n\n            with open(segment_file, \"wb\") as f:\n                f.write(response.content)\n\n            return segment_file\n\n        client = openai.AsyncOpenAI(\n            base_url=OPENAI_BASE_URL,\n            api_key=OPENAI_API_KEY,\n        )\n\n        response = await client.beta.chat.completions.parse(\n            model=\"gpt-5.2\",\n            response_format=ResponseFormat,\n            messages=[\n                openai.types.chat.ChatCompletionSystemMessageParam(\n                    role=\"system\",\n                    content=\"\"\"\n2人のパーソナリティ(タカシとタケシ)が登場するポッドキャストのスクリプトを日本語で作成してください。\n与えられた内容を面白く、魅力的に紹介する必要があります。\n\nタカシ: 冷静で分析的なタイプで、事実とデータを重視します。専門的な解説が得意です。\nタケシ: 明るく活発でユーモアのあるタイプです。感情表現が豊かで、聴衆を楽しませるのが得意です。\n\n各ホストの発言は明確に示してください。\nイントロ、メインコンテンツの紹介、そして締めくくりという構成にしてください。\n最後に、生成されたスクリプトをもとに興味を引くポッドキャストのタイトルも考えてください。\n\"\"\",\n                ),\n                openai.types.chat.ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=self.result.value,\n                )\n            ],\n        )\n\n        content = response.choices[0].message.content\n        response_format = ResponseFormat.model_validate_json(content)\n\n        with tempfile.TemporaryDirectory() as t:\n            dialogue_segments = [(segment.speaker, segment.content) for segment in response_format.dialogues]\n\n            tasks = []\n            for i, (speaker, text) in enumerate(dialogue_segments):\n                segment_file = os.path.join(t, f\"segment_{i}.mp3\")\n                tasks.append(generate_speech_segment(client, speaker, text, segment_file))\n\n            segment_files = await asyncio.gather(*tasks)\n\n            audio_segments = [pydub.AudioSegment.from_file(segment_file) for segment_file in segment_files]\n            silence = pydub.AudioSegment.silent(duration=300)\n            if audio_segments:\n                combined_audio = pydub.AudioSegment.empty()\n                for segment in audio_segments:\n                    if len(combined_audio) > 0:\n                        combined_audio += silence\n                    combined_audio += segment\n\n                combined_file = f\"/tmp/{response_format.title}.mp3\"\n                combined_audio.export(combined_file, format=\"mp3\")\n                IPython.display.display(IPython.display.Audio(combined_file))\n\n    @output.capture()\n    def podcast_handler(self, b: ipywidgets.widgets.Button) -> None:\n        b.disabled = True\n\n        task = asyncio.create_task(self._podcast_handler())\n        asyncio.get_running_loop().run_until_complete(task)\n\n        b.disabled = False\n\n\ntab_contents = {}\ntabs = ipywidgets.widgets.Tab()\n\n\ndef initialize_tabs():\n    tab_contents[COMBINED_TAB_NAME] = TabContent(COMBINED_TAB_NAME)\n    tabs.children = [tab_contents[COMBINED_TAB_NAME].widgets]\n    tabs.set_title(0, COMBINED_TAB_NAME)\n\n\ninitialize_tabs()\n\ndiarization_options = ipywidgets.widgets.VBox([\n    diarization,\n    ipywidgets.widgets.HBox([speakers])\n])\n\n\ndef update_speakers_availability(change) -> None:\n    speakers.disabled = not change.new\n\n\ndiarization.observe(update_speakers_availability, names=\"value\")\n\n\ndef update_diarization_availability(change) -> None:\n    enabled = True\n\n    for uploaded in change.new:\n        file_path = os.path.abspath(uploaded[\"name\"])\n        extension = os.path.splitext(file_path)[1].lower()\n        if extension not in [\".wav\", \".mp3\"]:\n            enabled = False\n            break\n\n    diarization.disabled = not enabled\n\n\nfile_upload.observe(update_diarization_availability, names=\"file_info\")\n\nIPython.display.display(\n    file_upload,\n    prompt,\n    model,\n    diarization_options,\n    process_button,\n    output,\n    status_label,\n    tabs,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e5c7a-692f-4571-a4f4-7288e1d825ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}